{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Develop PD model, by Brent Oeyen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this code is to compare the following Probability of Default models\n",
    "- An IRLS logistic regression with a classic transformation of variable;\n",
    "- An elastic net logistic regression with raw numeric input variables, calibrated with scipy minize;\n",
    "- A Keras DNN binary classification algorithm with raw numeric input variables transformed with a standard score; and\n",
    "- A Keras elastic net with raw numeric input variables transformed with a standard score.\n",
    "\n",
    "The following packages are used in the implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas                     as pd\n",
    "import numpy                      as np\n",
    "import scipy.optimize             as optimize\n",
    "import scipy.stats                as st\n",
    "from keras.models                 import Sequential\n",
    "from keras.layers                 import Dense\n",
    "from keras.wrappers.scikit_learn  import KerasClassifier\n",
    "from keras.layers                 import Dropout\n",
    "from keras.constraints            import maxnorm\n",
    "from keras.optimizers             import SGD\n",
    "from sklearn.model_selection      import cross_val_score\n",
    "from sklearn.preprocessing        import LabelEncoder\n",
    "from sklearn.model_selection      import StratifiedKFold\n",
    "from sklearn.preprocessing        import StandardScaler\n",
    "from sklearn.pipeline             import Pipeline\n",
    "from sklearn.calibration          import CalibratedClassifierCV\n",
    "from sklearn.linear_model         import LogisticRegression\n",
    "from sklearn.model_selection      import train_test_split\n",
    "from Codes.PD.PD_tests            import *\n",
    "from Codes.PD.Logistic_regression import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset, source: \"http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "location              = os.getcwd() + '/Data/german data numeric'\n",
    "dataframe             = pd.read_csv(location, header=None, delimiter=r\"\\s+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data set into input (X) input variables and binary (Y) output variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X                     = dataframe.values[:,0:24].astype(float)\n",
    "dataframe.iloc[:, 24] = dataframe.iloc[:, 24]==2\n",
    "Y                     = dataframe.values[:,24].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform input variables with a logit transformation per quantiles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(df, j):\n",
    " df.columns     = ['var', 'binary']\n",
    " try: \n",
    "  df['decile'] = pd.qcut(df['var'], q=np.arange(1, 51)/50, labels=False)\n",
    "  df           = pd.merge(df.astype(float), df.groupby('decile', as_index=False)['binary'].mean(), \\\n",
    "                          on='decile', how='inner')\n",
    " except:\n",
    "  df           = pd.merge(df.astype(float), df.groupby('var'   , as_index=False)['binary'].mean(), \\\n",
    "                          on='var'   , how='inner')\n",
    " pi            = df.iloc[:, -1].fillna(0.5).replace(0, 0.00001)\n",
    " return pd.DataFrame({'X'+str(j): np.log(pi/(1-pi)).replace(np.inf, 15)})\n",
    "X_scaled       = transform(dataframe.iloc[:, [0,24]], 1)\n",
    "for i in range(1, dataframe.shape[1]-1):\n",
    " X_scaled['X'+str(i+1)] = transform(dataframe.iloc[:, [i,24]], i+1)\n",
    "X_scaled       = X_scaled.fillna(0)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y, test_size=0.2, random_state=42)\n",
    "X_tr   , X_tes , Y_tr   , Y_tes  = train_test_split(X       , Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output variance of each transformed input variable as an indication of which variables contain a lot of information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1      0.762004\n",
      "X2      3.421997\n",
      "X3      0.295841\n",
      "X4     14.081804\n",
      "X5      0.226596\n",
      "X6      0.085566\n",
      "X7      0.043210\n",
      "X8      0.003601\n",
      "X9      0.110774\n",
      "X10     1.683886\n",
      "X11     0.053565\n",
      "X12     0.013477\n",
      "X13     0.000043\n",
      "X14     0.006430\n",
      "X15     0.056956\n",
      "X16     0.041302\n",
      "X17     0.065301\n",
      "X18     0.000003\n",
      "X19     0.014833\n",
      "X20     0.036471\n",
      "X21     0.079953\n",
      "X22     0.000149\n",
      "X23     0.002296\n",
      "X24     0.000869\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(X_train.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify whether they the transformation of variable was succesful by comparing the rank correlation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw input variable X2:  KendalltauResult(correlation=0.17609245525504136, pvalue=7.975280722434196e-11)\n",
      "Transformed input variable X2: KendalltauResult(correlation=0.013532833811191117, pvalue=0.6174826267368242)\n"
     ]
    }
   ],
   "source": [
    "print(\"Raw input variable X2: \", st.kendalltau(Y, X[:, 1]))\n",
    "print('Transformed input variable X2:', st.kendalltau(Y, X_scaled.iloc[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Master Scale of 22 ratings mapped to equidistant PDs in logit space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings            = 22                                         #Number of rating grades\n",
    "PD_min             = 0.0003                                     #Minimum PD value (regulatory threshold)\n",
    "slope              = (np.log((2-PD_min)/PD_min)-0)/(ratings-1)  #Slope between min PD value and default in logit space\n",
    "MS                 = 2/(1+np.exp(slope*pd.Series(list(range(ratings)))))\n",
    "idx                = pd.IntervalIndex.from_arrays(MS[1:].append(pd.Series(0)), MS, closed='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IRLS logistic regression with a classic transformation of variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calibrate model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IRLS converged after 4iterations.\n"
     ]
    }
   ],
   "source": [
    "betas_start              = np.append(np.log(Y_train.mean()/(1-Y_train.mean())), np.zeros(8))\n",
    "betas_IRLS, y_train_IRLS = logistic_regression().IRLS(betas_start, np.append(np.ones([len(X_train.index), 2]), \\\n",
    "                           X_train.iloc[:, np.r_[0:4, 7:9, 16, 20]], axis=1)[:, 1:], Y_train)\n",
    "X_IRLS                   = np.append(np.ones([len(X_scaled.index), 2]), \\\n",
    "                           X_scaled.iloc[:, np.r_[0:4, 7:9, 16, 20]], axis=1)[:, 1:]\n",
    "IRLS_all                 = pd.DataFrame({'PD': 1/(1+np.exp(X_IRLS.dot(-betas_IRLS)))}) \n",
    "IRLS_all['Y']            = Y\n",
    "IRLS_all['rating_PD']    = MS[idx.get_indexer(IRLS_all.PD)].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output results for a regression with the 8 highest correlated variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance metrics: [AUC all 53.59%]\n",
      "Coefficients regression: [ 0.63104141 -0.00249486 -0.02409827  0.21552432 -0.00565019  0.70273931\n",
      "  0.23757129  0.47568141  0.12372487]\n",
      "Jeffrey test\n",
      "                    rating_PD        PD         Y        H0     p_val\n",
      "                        count      mean      mean                    \n",
      "rating_PD                                                            \n",
      "0.21890161217657667       2.0  0.198208  0.500000  0.198208  0.140557\n",
      "0.31496223179993454     529.0  0.273935  0.270321  0.273935  0.571111\n",
      "0.4426995169025332      469.0  0.333599  0.332623  0.333599  0.515712\n",
      "Portfolio              1000.0  0.805742  1.102944  0.301766  0.546622\n"
     ]
    }
   ],
   "source": [
    "print(\"Performance metrics: [AUC all %.2f%%]\" % (PD_tests().AUC(IRLS_all.Y, IRLS_all.rating_PD,0)[0]*100))\n",
    "print(\"Coefficients regression:\", betas_IRLS)\n",
    "dummy               = PD_tests().Jeffrey(IRLS_all, 'rating_PD', 'PD', 'Y')\n",
    "print('Jeffrey test')\n",
    "print(dummy.iloc[:, np.r_[0, 3, 6,  10:12]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the poor performance of the scaled input variables, the raw input variables are used instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IRLS converged after 5iterations.\n",
      "Performance metrics: [AUC all 77.88%]\n",
      "Coefficients regression: [ 0.77847929 -0.56161866  0.0246094  -0.36420994  0.00514031 -0.0544452\n",
      "  0.20944244 -1.17560474 -0.51219642]\n",
      "Jeffrey test\n",
      "                     rating_PD        PD         Y        H0     p_val\n",
      "                         count      mean      mean                    \n",
      "rating_PD                                                             \n",
      "0.029760560578478736       4.0  0.027608  0.000000  0.027608  0.374643\n",
      "0.04491339548733918       26.0  0.039108  0.000000  0.039108  0.852174\n",
      "0.06751703372078455       63.0  0.056960  0.031746  0.056960  0.800781\n",
      "0.10090929621852611       96.0  0.083958  0.083333  0.083958  0.488103\n",
      "0.14953859341903025      115.0  0.125885  0.130435  0.125885  0.428017\n",
      "0.21890161217657667      135.0  0.184365  0.177778  0.184365  0.569288\n",
      "0.31496223179993454      162.0  0.263878  0.253086  0.263878  0.617424\n",
      "0.4426995169025332       163.0  0.377768  0.404908  0.377768  0.236454\n",
      "0.6036825140620321       128.0  0.526771  0.531250  0.526771  0.460227\n",
      "0.7933816324488658        90.0  0.688752  0.677778  0.688752  0.594176\n",
      "1.0                       18.0  0.850305  0.833333  0.850305  0.609116\n",
      "Portfolio               1000.0  3.225356  3.123647  0.301186  0.530763\n"
     ]
    }
   ],
   "source": [
    "betas_start              = np.append(np.log(Y_tr.mean()/(1-Y_tr.mean())), np.zeros(8))\n",
    "betas_IRLS, y_train_IRLS = logistic_regression().IRLS(betas_start, np.append(np.ones([len(X_tr), 2]), \\\n",
    "                           X_tr[:, np.r_[0:4, 7:9, 16, 20]], axis=1)[:, 1:], Y_tr)\n",
    "X_IRLS                   = np.append(np.ones([len(X), 2]), \\\n",
    "                           X[:, np.r_[0:4, 7:9, 16, 20]], axis=1)[:, 1:]\n",
    "IRLS_all                 = pd.DataFrame({'PD': 1/(1+np.exp(X_IRLS.dot(-betas_IRLS)))}) \n",
    "IRLS_all['Y']            = Y\n",
    "IRLS_all['rating_PD']    = MS[idx.get_indexer(IRLS_all.PD)].values\n",
    "print(\"Performance metrics: [AUC all %.2f%%]\" % (PD_tests().AUC(IRLS_all.Y, IRLS_all.rating_PD,0)[0]*100))\n",
    "print(\"Coefficients regression:\", betas_IRLS)\n",
    "dummy               = PD_tests().Jeffrey(IRLS_all, 'rating_PD', 'PD', 'Y')\n",
    "print('Jeffrey test')\n",
    "print(dummy.iloc[:, np.r_[0, 3, 6,  10:12]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic net logistic regression with raw numeric input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution             = optimize.minimize(fun=logistic_regression().el_logicreg, x0=betas_IRLS, args=(Y_tr, \\\n",
    "                       X_tr[:, np.r_[0:4, 7:9, 16, 20]], 0.3, 0.5), method='Nelder-Mead', options={\"maxiter\":5000})\n",
    "X_LL                 = np.append(np.ones([len(X), 2]), X[:, np.r_[0:4, 7:9, 16, 20]], axis=1)[:, 1:]\n",
    "LL_all               = pd.DataFrame({'PD': 1/(1+np.exp(X_LL.dot(-solution.x)))}) \n",
    "LL_all['Y']          = Y\n",
    "LL_all['rating_PD']  = MS[idx.get_indexer(LL_all.PD)].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output results for a regression with the 8 highest correlated variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results logistic regression ML: Elastic net (Lambda=0.3, L1 ratio=0.5)\n",
      "Performance metrics: [AUC all 72.35%]\n",
      "Coefficients regression: [ 1.34782851e+00 -3.40238037e-01  5.28438161e-02 -4.65549331e-01\n",
      " -1.58887734e-02 -1.50196892e-01 -1.39382220e-04  9.86211126e-10\n",
      " -1.51418317e-01]\n",
      "Jeffrey test\n",
      "                    rating_PD        PD         Y        H0     p_val\n",
      "                        count      mean      mean                    \n",
      "rating_PD                                                            \n",
      "0.04491339548733918       3.0  0.036647  0.333333  0.036647  0.023039\n",
      "0.06751703372078455       5.0  0.063221  0.400000  0.063221  0.009732\n",
      "0.10090929621852611      28.0  0.091162  0.035714  0.091162  0.849184\n",
      "0.14953859341903025      87.0  0.123939  0.080460  0.123939  0.895987\n",
      "0.21890161217657667     162.0  0.185623  0.154321  0.185623  0.847680\n",
      "0.31496223179993454     184.0  0.263588  0.228261  0.263588  0.862264\n",
      "0.4426995169025332      204.0  0.376796  0.225490  0.376796  0.999998\n",
      "0.6036825140620321      195.0  0.514633  0.482051  0.514633  0.818697\n",
      "0.7933816324488658      119.0  0.685217  0.605042  0.685217  0.968352\n",
      "1.0                      13.0  0.841622  0.769231  0.841622  0.776087\n",
      "Portfolio              1000.0  3.182449  3.313904  0.362034  0.999982\n"
     ]
    }
   ],
   "source": [
    "print(\"Results logistic regression ML: Elastic net (Lambda=0.3, L1 ratio=0.5)\")\n",
    "print(\"Performance metrics: [AUC all %.2f%%]\" % (PD_tests().AUC(LL_all.Y, LL_all.rating_PD,0)[0]*100))\n",
    "print(\"Coefficients regression:\", solution.x)\n",
    "dummy               = PD_tests().Jeffrey(LL_all, 'rating_PD', 'PD', 'Y')\n",
    "print('Jeffrey test')\n",
    "print(dummy.iloc[:, np.r_[0, 3, 6,  10:12]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras DNN with raw numeric input variables transformed with a standard score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_binary():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(24, input_dim=24, activation='relu'))\n",
    "\tmodel.add(Dropout(0.2))\n",
    "\tmodel.add(Dense(24, activation='relu'))\n",
    "\tmodel.add(Dropout(0.2))\n",
    "\tmodel.add(Dense(1, activation='sigmoid'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model\n",
    "estimators          = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=create_binary, epochs=80, batch_size=16, verbose=0)))\n",
    "pipeline            = Pipeline(estimators)\n",
    "pipeline.fit(X_tr, Y_tr)\n",
    "kfold               = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "results             = cross_val_score(pipeline, X, Y, cv=kfold)\n",
    "b_pred              = pd.DataFrame(pipeline.predict_proba(X_tes))\n",
    "b_pred.columns      = ['ID', 'PD']\n",
    "b_pred['Y']         = Y_tes\n",
    "b_pred['rating_PD'] = MS[idx.get_indexer(b_pred.PD)].values\n",
    "b_all               = pd.DataFrame(pipeline.predict_proba(X))\n",
    "b_all.columns       = ['ID', 'PD']\n",
    "b_all['Y']          = Y\n",
    "b_all['rating_PD']  = MS[idx.get_indexer(b_all.PD)].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output results with all input variables being used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance metrics: Mean Accuracy CV 74.90% (STD 2.88%) [AUC test 78.31%] [AUC all 92.09%]\n",
      "Jeffrey test\n",
      "                       rating_PD        PD         Y        H0     p_val\n",
      "                           count      mean      mean                    \n",
      "rating_PD                                                               \n",
      "0.00029999999999999976      27.0  0.000120  0.000000  0.000120  0.064528\n",
      "0.0004562208354469178        5.0  0.000394  0.000000  0.000394  0.051319\n",
      "0.0006937632793232519       14.0  0.000580  0.000000  0.000580  0.102292\n",
      "0.0010549227058496641       15.0  0.000922  0.000000  0.000922  0.133248\n",
      "0.0016039437457623122       14.0  0.001259  0.000000  0.001259  0.150303\n",
      "0.002438347229665251        24.0  0.001964  0.041667  0.001964  0.007378\n",
      "0.0037060190379897217       20.0  0.003030  0.000000  0.003030  0.274099\n",
      "0.005630882685189334        19.0  0.004764  0.052632  0.004764  0.019101\n",
      "0.00855121588022675         28.0  0.007344  0.035714  0.007344  0.061640\n",
      "0.012976261070719905        23.0  0.010860  0.043478  0.010860  0.080403\n",
      "0.01966854325104041         30.0  0.016925  0.033333  0.016925  0.202517\n",
      "0.029760560578478736        37.0  0.025303  0.000000  0.025303  0.832971\n",
      "0.04491339548733918         44.0  0.036913  0.045455  0.036913  0.338925\n",
      "0.06751703372078455         67.0  0.055981  0.059701  0.055981  0.416403\n",
      "0.10090929621852611         63.0  0.084232  0.031746  0.084232  0.947688\n",
      "0.14953859341903025         60.0  0.124943  0.133333  0.124943  0.403778\n",
      "0.21890161217657667         79.0  0.180652  0.088608  0.180652  0.988616\n",
      "0.31496223179993454         79.0  0.266653  0.215190  0.266653  0.850006\n",
      "0.4426995169025332          84.0  0.373171  0.416667  0.373171  0.204085\n",
      "0.6036825140620321          82.0  0.516930  0.609756  0.516930  0.045744\n",
      "0.7933816324488658          87.0  0.695557  0.885057  0.695557  0.000015\n",
      "1.0                         99.0  0.885664  0.939394  0.885664  0.037534\n",
      "Portfolio                 1000.0  3.294161  3.631731  0.277586  0.057593\n"
     ]
    }
   ],
   "source": [
    "print(\"Performance metrics: Mean Accuracy CV %.2f%% (STD %.2f%%) [AUC test %.2f%%] [AUC all %.2f%%]\" \\\n",
    "% (results.mean()*100, results.std()*100, PD_tests().AUC(b_pred.Y, b_pred.rating_PD,0)[0]*100, \\\n",
    "   PD_tests().AUC(b_all.Y, b_all.rating_PD,0)[0]*100))\n",
    "dummy               = PD_tests().Jeffrey(b_all, 'rating_PD', 'PD', 'Y')\n",
    "print('Jeffrey test')\n",
    "print(dummy.iloc[:, np.r_[0, 3, 6,  10:12]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras elastic net with raw numeric input variables transformed with a standard score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR                  = LogisticRegression(C=0.3, penalty='elasticnet', solver='saga', l1_ratio=0.5, \\\n",
    "                                         max_iter=1000, tol=0.001)\n",
    "scaler              = StandardScaler()\n",
    "scaler.fit(X_tr)\n",
    "LR.fit(scaler.transform(X_tr), Y_tr)\n",
    "LR_pred             = pd.DataFrame(LR.predict_proba(scaler.transform(X_tes)))\n",
    "LR_pred.columns     = ['ID', 'PD']\n",
    "LR_pred['Y']        = Y_tes\n",
    "LR_pred['rating_PD']= MS[idx.get_indexer(LR_pred.PD)].values\n",
    "LR_all              = pd.DataFrame(LR.predict_proba(scaler.transform(X)))\n",
    "LR_all.columns      = ['ID', 'PD']\n",
    "LR_all['Y']         = Y \n",
    "LR_all['rating_PD'] = MS[idx.get_indexer(LR_all.PD)].values\n",
    "results             = cross_val_score(LR, X, Y, cv=kfold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output results with all input variables being used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results logistic regression: Elastic net (Lambda=0.3, L1 ratio=0.5)\n",
      "Performance metrics: Mean Accuracy CV 75.10% (STD 2.84%) [AUC test 80.51%] [AUC all 81.15%]\n",
      "Coefficients regression: [[-0.65714083  0.30592054 -0.39515111  0.15154385 -0.23556821 -0.15974222\n",
      "  -0.11155633  0.01905668  0.16390315 -0.25583818 -0.22045609  0.14207899\n",
      "   0.         -0.07077531 -0.21050127  0.28600726 -0.2452701   0.20259282\n",
      "   0.10702668  0.01964218 -0.19007338 -0.10013797 -0.04893568  0.        ]]\n",
      "Jeffrey test\n",
      "                     rating_PD        PD         Y        H0     p_val\n",
      "                         count      mean      mean                    \n",
      "rating_PD                                                             \n",
      "0.00855121588022675        1.0  0.006156  0.000000  0.006156  0.099792\n",
      "0.012976261070719905       2.0  0.012371  0.000000  0.012371  0.187656\n",
      "0.01966854325104041        6.0  0.016613  0.000000  0.016613  0.352909\n",
      "0.029760560578478736      21.0  0.024908  0.000000  0.024908  0.699527\n",
      "0.04491339548733918       40.0  0.036768  0.000000  0.036768  0.917541\n",
      "0.06751703372078455       61.0  0.056392  0.049180  0.056392  0.564286\n",
      "0.10090929621852611       91.0  0.084324  0.065934  0.084324  0.725217\n",
      "0.14953859341903025      105.0  0.122635  0.142857  0.122635  0.256485\n",
      "0.21890161217657667      145.0  0.182612  0.151724  0.182612  0.831874\n",
      "0.31496223179993454      130.0  0.261591  0.253846  0.261591  0.573564\n",
      "0.4426995169025332       128.0  0.373937  0.359375  0.373937  0.630607\n",
      "0.6036825140620321       134.0  0.518178  0.567164  0.518178  0.128114\n",
      "0.7933816324488658       106.0  0.687456  0.735849  0.687456  0.140659\n",
      "1.0                       30.0  0.843433  0.700000  0.843433  0.978151\n",
      "Portfolio               1000.0  3.227374  3.025930  0.302073  0.555003\n"
     ]
    }
   ],
   "source": [
    "print(\"Results logistic regression: Elastic net (Lambda=0.3, L1 ratio=0.5)\")\n",
    "print(\"Performance metrics: Mean Accuracy CV %.2f%% (STD %.2f%%) [AUC test %.2f%%] [AUC all %.2f%%]\" \\\n",
    "% (results.mean()*100, results.std()*100, PD_tests().AUC(LR_pred.Y, LR_pred.rating_PD,0)[0]*100, \\\n",
    "   PD_tests().AUC(LR_all.Y, LR_all.rating_PD,0)[0]*100))\n",
    "print(\"Coefficients regression:\", LR.coef_)\n",
    "dummy               = PD_tests().Jeffrey(LR_all, 'rating_PD', 'PD', 'Y')\n",
    "print('Jeffrey test')\n",
    "print(dummy.iloc[:, np.r_[0, 3, 6,  10:12]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Based on the results of the credit data set, transforming the input data wasn't successful on a small sample population (1K). The imbalance of the observed defaults and performing credit did not create calibration issues.\n",
    "\n",
    "The Keras library is straight forward to use and requires little development from the user. Albeit fine tuning the configuration parameters to run the models will require some time and experience with both the tool and modelling a given dependent variable. In addition, the Keras library outperforms a cinch implementation of the logistic regression's IRLS and ML elastic net method.\n",
    "The DNN model outperforms the logistic regression based on the AUC of the total population (training and test data) but doesn't on the training set. In addition, the Cross Validation (CV) score of the training set is comparable while the logistic regression produces more stable results.\n",
    "\n",
    "Considering the comparable performance between DNN and elastic net and the apprehensible results of the elastic net in contrast of the complexity of understanding the probability calculation of the DNN algorithm (not covered in this example), logistic regression with an elastic net is a preferred option for PD models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
